{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "from classes import AudioModel, ImprovedAudioModel, AudioTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERS = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_spectrogram(file_path: str):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "    transformer = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sr, n_fft=2048, hop_length=512, n_mels=128\n",
    "        )\n",
    "\n",
    "    spectrogram = transformer(waveform)\n",
    "    spectrogram = torchaudio.transforms.AmplitudeToDB()(spectrogram)\n",
    "    return spectrogram.squeeze(0).transpose(0, 1)\n",
    "\n",
    "\n",
    "def vector_quantize(features, n_clusters: int = 100):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    all_data = np.vstack([f.numpy() for f in features])\n",
    "    kmeans.fit(all_data)\n",
    "\n",
    "    quantized_features = [\n",
    "        torch.tensor(kmeans.predict(f.numpy()), dtype=torch.long)\n",
    "        for f in features\n",
    "        ]\n",
    "\n",
    "    return quantized_features, kmeans\n",
    "\n",
    "\n",
    "# def load_and_quantize_data(\n",
    "#         directory: str,\n",
    "#         target_labels: list[str] = [\"up\", \"down\", \"left\", \"right\"],\n",
    "#         n_clusters: int = 100\n",
    "#         ) -> tuple:\n",
    "\n",
    "#     features = []\n",
    "#     labels = []\n",
    "#     for label in os.listdir(directory):\n",
    "#         if target_labels is None or label in target_labels:\n",
    "#             class_dir = os.path.join(directory, label)\n",
    "#             if \".gitkeep\" not in class_dir:\n",
    "#                 for fname in os.listdir(class_dir):\n",
    "#                     file_path = os.path.join(class_dir, fname)\n",
    "#                     spectrogram = audio_to_spectrogram(file_path)\n",
    "#                     features.append(spectrogram)\n",
    "#                     labels.append(label)\n",
    "\n",
    "#     quantized_features, kmeans = vector_quantize(features, n_clusters)\n",
    "\n",
    "#     return quantized_features, labels, kmeans\n",
    "\n",
    "\n",
    "def load_and_quantize_data(\n",
    "        directory: str,\n",
    "        target_labels: list[str] = [\"up\", \"down\", \"left\", \"right\"],\n",
    "        n_clusters: int = 100\n",
    "        ) -> tuple:\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    for label in os.listdir(directory):\n",
    "        class_dir = os.path.join(directory, label)\n",
    "        # Sprawdź, czy folder nie zawiera nazwy pliku specjalnego\n",
    "        if os.path.isdir(class_dir) and \".gitkeep\" not in class_dir:\n",
    "            # Przejdź po wszystkich plikach w folderze\n",
    "            for fname in os.listdir(class_dir):\n",
    "                file_path = os.path.join(class_dir, fname)\n",
    "                spectrogram = audio_to_spectrogram(file_path)\n",
    "                features.append(spectrogram)\n",
    "                # Jeśli etykieta należy do docelowych, użyj tej etykiety\n",
    "                if label in target_labels:\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    # W przeciwnym razie przypisz etykietę \"unknown\"\n",
    "                    labels.append(\"unknown\")\n",
    "\n",
    "    # Przeprowadź kwantyzację cech\n",
    "    quantized_features, kmeans = vector_quantize(features, n_clusters)\n",
    "\n",
    "    return quantized_features, labels, kmeans\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_value: int = 0):\n",
    "    max_len = max([s.size(0) for s in sequences])\n",
    "\n",
    "    padded_sequences = [\n",
    "        torch.nn.functional.pad(s, (0, max_len - s.size(0)), value=pad_value)\n",
    "        for s in sequences\n",
    "        ]\n",
    "\n",
    "    return torch.stack(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, criterion) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            labels = labels.long()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Studia\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "if Path(\"models/kmeans_model.joblib\").exists():\n",
    "    kmeans = joblib.load(\"models/kmeans_model.joblib\")\n",
    "\n",
    "else:\n",
    "    features, labels, kmeans = load_and_quantize_data(\n",
    "        \"data/train\",\n",
    "        n_clusters=CLUSTERS,\n",
    "        target_labels=[\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]\n",
    "        )\n",
    "\n",
    "    joblib.dump(kmeans, \"models/kmeans_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_padded = pad_sequences(features)\n",
    "\n",
    "train_features_padded, test_features_padded, train_labels, test_labels = train_test_split(\n",
    "    features_padded, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "\n",
    "if Path(\"models/label_encoder.joblib\").exists():\n",
    "    label_encoder = joblib.load(\"models/label_encoder.joblib\")\n",
    "\n",
    "else:\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = torch.tensor(label_encoder.fit_transform(train_labels))\n",
    "    test_labels_encoded = torch.tensor(label_encoder.transform(test_labels))\n",
    "\n",
    "    joblib.dump(label_encoder, \"models/label_encoder.joblib\")\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_features_padded, train_labels_encoded)\n",
    "test_dataset = TensorDataset(test_features_padded, test_labels_encoded)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(train_labels))\n",
    "\n",
    "models = {\n",
    "    AudioTransformer: {\n",
    "        \"num_tokens\": CLUSTERS,\n",
    "        \"num_classes\": num_classes\n",
    "    },\n",
    "    AudioModel: {\n",
    "        \"num_tokens\": CLUSTERS,\n",
    "        \"dim_model\": 256,\n",
    "        \"num_heads\": 16,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"dim_feedforward\": 4096,\n",
    "        \"num_layers\": 2\n",
    "    },\n",
    "    ImprovedAudioModel: {\n",
    "        \"num_tokens\": CLUSTERS,\n",
    "        \"num_classes\": num_classes\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader) -> dict:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    loss_dict = {\n",
    "        \"test\": [],\n",
    "        \"train\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        loss_sum = []\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum.append(loss.item())\n",
    "        \n",
    "        test_loss, test_accuracy = eval_model(model, test_loader, criterion)\n",
    "\n",
    "        loss_ = np.mean(np.array(loss_sum))\n",
    "\n",
    "        loss_dict[\"test\"].append(test_loss)\n",
    "        loss_dict[\"train\"].append(loss_)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {round(loss_, 4)} Test loss: {round(test_loss, 4)} Test acc: {round(test_accuracy, 2):4}\")\n",
    "        if len(loss_dict[\"test\"]) > 1 and loss_dict[\"test\"][-2] <= loss_dict[\"test\"][-1]:\n",
    "            break\n",
    "\n",
    "        torch.save(model.state_dict(), f\"models/{model.name}_{epoch}.pth\")\n",
    "    \n",
    "    return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioTransformer\n",
      "Epoch 1, Loss: 1.3187 Test loss: 1.2901 Test acc: 62.47\n",
      "Epoch 2, Loss: 1.2833 Test loss: 1.2725 Test acc: 62.8\n",
      "Epoch 3, Loss: 1.2788 Test loss: 1.3035 Test acc: 63.51\n",
      "\n",
      "AudioModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Studia\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.521 Test loss: 1.4923 Test acc: 63.41\n",
      "Epoch 2, Loss: 1.4955 Test loss: 1.5132 Test acc: 63.41\n",
      "\n",
      "ImprovedAudioModel\n",
      "Epoch 1, Loss: 0.9736 Test loss: 0.6808 Test acc: 78.22\n",
      "Epoch 2, Loss: 0.6314 Test loss: 0.5834 Test acc: 81.29\n",
      "Epoch 3, Loss: 0.5502 Test loss: 0.5615 Test acc: 82.11\n",
      "Epoch 4, Loss: 0.5033 Test loss: 0.5604 Test acc: 81.57\n",
      "Epoch 5, Loss: 0.4667 Test loss: 0.5535 Test acc: 82.55\n",
      "Epoch 6, Loss: 0.4394 Test loss: 0.547 Test acc: 82.74\n",
      "Epoch 7, Loss: 0.422 Test loss: 0.5471 Test acc: 82.99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_type in models:\n",
    "    model_args = models[model_type]\n",
    "    model = model_type(**model_args)\n",
    "\n",
    "    print(model_type.name)\n",
    "\n",
    "    train_model(model, train_loader, test_loader)\n",
    "\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
