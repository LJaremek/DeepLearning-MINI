{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import sounddevice as sd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_tokens: int, dim_model: int, num_heads: int,\n",
    "            num_classes: int, dim_feedforward: int = 2048,\n",
    "            num_layers: int = 1, dropout: int = 0.1\n",
    "            ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_model, nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward, dropout=dropout\n",
    "            )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers\n",
    "            )\n",
    "\n",
    "        self.fc = nn.Linear(dim_model, num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src)  # Replace tokens with embeddings\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(duration=1.5, sr=16000):\n",
    "    audio_data = sd.rec(\n",
    "        int(duration * sr),\n",
    "        samplerate=sr,\n",
    "        channels=1,\n",
    "        dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "    sd.wait()\n",
    "    return audio_data.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Studia\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioTransformer(\n",
       "  (embedding): Embedding(100, 256)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = 100  # Same as number of clusters\n",
    "\n",
    "model = AudioTransformer(\n",
    "    num_tokens=num_tokens,\n",
    "    dim_model=256,\n",
    "    num_heads=8,\n",
    "    num_classes=4\n",
    "    )\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/model.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sounddevice as sd\n",
    "\n",
    "\n",
    "# def record_audio(duration=5, sr=16000):\n",
    "#     print(\"Nagrywanie rozpoczęte...\")\n",
    "#     audio_data = sd.rec(int(duration * sr), samplerate=sr, channels=1, dtype='float32')\n",
    "#     sd.wait()\n",
    "#     print(\"Nagrywanie zakończone.\")\n",
    "#     return audio_data.flatten()\n",
    "\n",
    "\n",
    "# def play_audio(audio_data, sr=16000):\n",
    "#     print(\"Odtwarzanie dźwięku...\")\n",
    "#     sd.play(audio_data, samplerate=sr)\n",
    "#     sd.wait()\n",
    "#     print(\"Odtwarzanie zakończone.\")\n",
    "\n",
    "\n",
    "# # Nagrywanie dźwięku\n",
    "# audio_input = record_audio()\n",
    "\n",
    "# # Odtwarzanie nagranego dźwięku\n",
    "# play_audio(audio_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     audio_input \u001b[38;5;241m=\u001b[39m \u001b[43mrecord_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     audio_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(audio_input)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m, in \u001b[0;36mrecord_audio\u001b[1;34m(duration, sr)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecord_audio\u001b[39m(duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m):\n\u001b[0;32m      6\u001b[0m     audio_data \u001b[38;5;241m=\u001b[39m sd\u001b[38;5;241m.\u001b[39mrec(\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mint\u001b[39m(duration \u001b[38;5;241m*\u001b[39m sr),\n\u001b[0;32m      8\u001b[0m         samplerate\u001b[38;5;241m=\u001b[39msr,\n\u001b[0;32m      9\u001b[0m         channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     10\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m         )\n\u001b[1;32m---> 13\u001b[0m     \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m audio_data\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\Studia\\anaconda3\\lib\\site-packages\\sounddevice.py:395\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(ignore_errors)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for `play()`/`rec()`/`playrec()` to be finished.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03mPlayback/recording can be stopped with a `KeyboardInterrupt`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m \n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _last_callback:\n\u001b[1;32m--> 395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_last_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Studia\\anaconda3\\lib\\site-packages\\sounddevice.py:2601\u001b[0m, in \u001b[0;36m_CallbackContext.wait\u001b[1;34m(self, ignore_errors)\u001b[0m\n\u001b[0;32m   2595\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for finished_callback.\u001b[39;00m\n\u001b[0;32m   2596\u001b[0m \n\u001b[0;32m   2597\u001b[0m \u001b[38;5;124;03mCan be interrupted with a KeyboardInterrupt.\u001b[39;00m\n\u001b[0;32m   2598\u001b[0m \n\u001b[0;32m   2599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2600\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2601\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2603\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mclose(ignore_errors)\n",
      "File \u001b[1;32mc:\\Users\\Studia\\anaconda3\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\Studia\\anaconda3\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    audio_input = record_audio()\n",
    "    audio_tensor = torch.tensor(audio_input).unsqueeze(0).long()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(audio_tensor)\n",
    "\n",
    "    predicted_command = output.argmax().item()\n",
    "\n",
    "    print(predicted_command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: ['up'], Probabilities: tensor([[0.2671, 0.2788, 0.0193, 0.4348]])\n"
     ]
    }
   ],
   "source": [
    "def audio_to_spectrogram(file_path: str):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "    transformer = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sr, n_fft=2048, hop_length=512, n_mels=64\n",
    "        )\n",
    "\n",
    "    spectrogram = transformer(waveform)\n",
    "    spectrogram = torchaudio.transforms.AmplitudeToDB()(spectrogram)\n",
    "    return spectrogram.squeeze(0).transpose(0, 1)\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, pad_value: int = 0):\n",
    "    max_len = max([s.size(0) for s in sequences])\n",
    "\n",
    "    padded_sequences = [\n",
    "        torch.nn.functional.pad(s, (0, max_len - s.size(0)), value=pad_value)\n",
    "        for s in sequences\n",
    "        ]\n",
    "\n",
    "    return torch.stack(padded_sequences)\n",
    "\n",
    "\n",
    "def vector_quantize(features, n_clusters: int = 100):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    all_data = np.vstack([f.numpy() for f in features])\n",
    "    kmeans.fit(all_data)\n",
    "\n",
    "    quantized_features = [\n",
    "        torch.tensor(kmeans.predict(f.numpy()), dtype=torch.long)\n",
    "        for f in features\n",
    "        ]\n",
    "\n",
    "    return quantized_features, kmeans\n",
    "\n",
    "\n",
    "def predict_single_file(file_path: str, model, kmeans, label_encoder):\n",
    "    spectrogram = audio_to_spectrogram(file_path)\n",
    "\n",
    "    all_data = np.vstack([spectrogram.numpy()])\n",
    "    quantized_features = torch.tensor(\n",
    "        kmeans.predict(all_data), dtype=torch.long\n",
    "        )\n",
    "\n",
    "    quantized_features_padded = pad_sequences([quantized_features])\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(quantized_features_padded)\n",
    "        predicted_probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        predicted_index = predicted_probabilities.argmax(1)\n",
    "        predicted_label = label_encoder.inverse_transform([predicted_index.item()])\n",
    "\n",
    "    return predicted_label, predicted_probabilities\n",
    "\n",
    "\n",
    "kmeans = load(\"models/kmeans_model.joblib\")\n",
    "label_encoder = load(\"models/label_encoder.joblib\")\n",
    "\n",
    "file_path = \"data/train/up/44260689_nohash_0.wav\"\n",
    "predicted_label, probabilities = predict_single_file(file_path, model, kmeans, label_encoder)\n",
    "print(f\"Predicted Label: {predicted_label}, Probabilities: {probabilities}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
